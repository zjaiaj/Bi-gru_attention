{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "NUM_WORDS = 10000\n",
    "INDEX_FROM = 3\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"D:/kesai/textmining/data/imdb.npz\",\n",
    "                                                      num_words=NUM_WORDS, index_from=INDEX_FROM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列化预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary_size(X):\n",
    "    return max([max(x) for x in X]) + 1\n",
    "\n",
    "vocabulay_size = get_vocabulary_size(X_train)\n",
    "print(vocabulay_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_in_vocabulary(X, voc_size):\n",
    "    return [[w for w in x if w < voc_size] for x in X]\n",
    "\n",
    "X_test = fit_in_vocabulary(X_test, vocabulay_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 250)\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 250\n",
    "\n",
    "def zero_pad(X, seq_len):\n",
    "    return np.array([x[:seq_len - 1] + [0] * max(seq_len - len(x), 1) for x in X])\n",
    "\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Inputs\"):\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name=\"batch_ph\")\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name=\"target_ph\")\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name=\"seq_len_ph\")\n",
    "    keeb_prob_ph = tf.placeholder(tf.float32, name=\"keep_prob_ph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda2\\envs\\py3.5\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Embedding层\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "with tf.name_scope(\"Embedding_layer\"):\n",
    "    embedding_var = tf.Variable(tf.random_uniform([vocabulay_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "    tf.summary.histogram(\"embedding\", embedding_var)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embedding_var, batch_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-11d100667e0b>:3: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-29-11d100667e0b>:4: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Anaconda2\\envs\\py3.5\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Anaconda2\\envs\\py3.5\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_outputs:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Bi-)RNN 层\n",
    "HIDDEN_SIZE = 150\n",
    "\n",
    "rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
    "tf.summary.histogram(\"RNN_outputs\", rnn_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \n",
    "    if isinstance(inputs, tuple):\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "        \n",
    "    if time_major:\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "    \n",
    "    hidden_size = inputs.shape[2].value\n",
    "    \n",
    "    # 训练参数\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    \n",
    "    with tf.name_scope('v'):\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "        \n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name=\"vu\")\n",
    "    alphas = tf.nn.softmax(vu, name=\"alphas\")\n",
    "    \n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "    \n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_SIZE = 50\n",
    "\n",
    "with tf.name_scope(\"Attention_layer\"):\n",
    "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    tf.summary.histogram(\"alphas\", alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-531061957afa>:2: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Droput\n",
    "drop = tf.nn.dropout(attention_output, keeb_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全连接层\n",
    "HIDDEN_SIZE = 150\n",
    "\n",
    "with tf.name_scope(\"Fully_connected_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0, shape=[1], dtype=tf.float32))\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    tf.summary.histogram(\"W\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Metrics\"):\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "    \n",
    "    # 准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size):\n",
    "    size = X.shape[0]\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    X_copy = X_copy[indices]\n",
    "    y_copy = y_copy[indices]\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i + batch_size <= size:\n",
    "            yield X_copy[i:i+batch_size], y_copy[i:i+batch_size]\n",
    "            i += batch_size\n",
    "        else:\n",
    "            i = 0\n",
    "            indices = np.arange(size)\n",
    "            np.random.shuffle(indices)\n",
    "            X_copy = X_copy[indices]\n",
    "            y_copy = y_copy[indices]\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter(\"./logdir/train\", accuracy.graph)\n",
    "test_writer = tf.summary.FileWriter(\"./logdir/test\", accuracy.graph)\n",
    "\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n",
      "epoch: 0\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 97/97 [11:25<00:00,  7.06s/it]\n",
      "100%|██████████████████████████████████████████| 97/97 [04:22<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.432, val_loss:0.424, acc:0.713,val_loss:0.804\n",
      "epoch: 1\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 97/97 [08:51<00:00,  5.48s/it]\n",
      "100%|██████████████████████████████████████████| 97/97 [04:10<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.320, val_loss:0.351, acc:0.846,val_loss:0.845\n",
      "epoch: 2\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 97/97 [09:34<00:00,  5.93s/it]\n",
      "100%|██████████████████████████████████████████| 97/97 [04:13<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.231, val_loss:0.328, acc:0.891,val_loss:0.859\n",
      "Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "MODEL_PATH = './model'\n",
    "KEEP_PROB = 0.8\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "            \n",
    "            print(\"epoch: {}\\t\".format(epoch+1), end=\"\")\n",
    "            \n",
    "            # 训练\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch ])\n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer ,merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keeb_prob_ph: KEEP_PROB})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "            \n",
    "            # Testing\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keeb_prob_ph: 1.0})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "            \n",
    "            print(\"loss:{:.3f}, val_loss:{:.3f}, acc:{:.3f}, val_loss:{:.3f}\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test))\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir ./ --host=127.0.0.1' to checkout tensorboard logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.5]",
   "language": "python",
   "name": "conda-env-py3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
